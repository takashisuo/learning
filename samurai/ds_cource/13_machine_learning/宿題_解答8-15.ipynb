{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"history_visible":true,"collapsed_sections":["hKIRnhjQyfxc","QNxf0HKeylWL","kc-_NgcizXyf","9rZSEERM2Tuh","2V6RbZQ85WSs","duShAMiu5Z2u"],"authorship_tag":"ABX9TyMDEO4buOxyOx3ULML2y33R"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["#実践8"],"metadata":{"id":"hKIRnhjQyfxc"}},{"cell_type":"markdown","source":["## 実践8-1\n","* irisデータ（非常に有名な花の種類(アヤメ)のデータセット）を用いて基礎集計をしてください。\n","* 基礎集計からこのデータの説明をしてください。\n","* 基礎集計後、train, testに分割しtestデータを推論してください。\n","* データの読み込みは以下です。\n","```\n","from sklearn.datasets import load_iris\n","iris = load_iris()\n","X, y = iris.data, iris.target\n","```\n","* モデルはDecisionTreeClassifierと課題と同じランダムフォレストの２種類を使ってください。\n","* accuracyを算出してください。"],"metadata":{"id":"lcl2XSf2yNMi"}},{"cell_type":"code","source":["from sklearn.datasets import load_iris\n","from sklearn.tree import DecisionTreeClassifier\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import accuracy_score\n","\n","# irisデータセットをロードする\n","iris = load_iris()\n","X, y = iris.data, iris.target\n","\n","# 訓練データとテストデータに分割する\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n","\n","# DecisionTreeClassifierを作成する\n","clf = DecisionTreeClassifier()\n","\n","# 訓練データを使って学習する\n","clf.fit(X_train, y_train)\n","\n","# テストデータで予測する\n","y_pred = clf.predict(X_test)\n","\n","# 正解率を計算する\n","accuracy = accuracy_score(y_test, y_pred)\n","print(f\"Accuracy: {accuracy}\")"],"metadata":{"id":"CdB8hfixz6Ne"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from sklearn.datasets import load_iris\n","from sklearn.ensemble import RandomForestClassifier\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import accuracy_score\n","\n","# irisデータセットをロードする\n","iris = load_iris()\n","X, y = iris.data, iris.target\n","\n","# 訓練データとテストデータに分割する\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n","\n","# RandomForestClassifierを作成する\n","clf = RandomForestClassifier()\n","\n","# 訓練データを使って学習する\n","clf.fit(X_train, y_train)\n","\n","# テストデータで予測する\n","y_pred = clf.predict(X_test)\n","\n","# 正解率を計算する\n","accuracy = accuracy_score(y_test, y_pred)\n","print(f\"Accuracy: {accuracy}\")"],"metadata":{"id":"_0AI29kv19-Z"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## 実践8-2\n","* digitsデータ(手書き文字の分類データ)を読み込んでください。\n","```\n","from sklearn.datasets import load_digits\n","digits = load_digits()\n","X, y = digits.data, digits.target\n","```\n","* iris同様、決定木とランダムフォレストのモデルを用いて推論してください。\n","* 正解率(accuracy)を算出してください。\n"],"metadata":{"id":"uxzr4YXWya6-"}},{"cell_type":"code","source":["from sklearn.datasets import load_digits\n","from sklearn.tree import DecisionTreeClassifier\n","from sklearn.ensemble import RandomForestClassifier\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import accuracy_score\n","\n","# digitsデータを読み込む\n","digits = load_digits()\n","X, y = digits.data, digits.target\n","\n","# データを訓練データとテストデータに分割\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n","\n","# 決定木モデルを作成して訓練\n","dt_model = DecisionTreeClassifier()\n","dt_model.fit(X_train, y_train)\n","\n","# 決定木モデルの予測値を取得し、正解率を算出\n","dt_pred = dt_model.predict(X_test)\n","dt_accuracy = accuracy_score(y_test, dt_pred)\n","print(f\"決定木モデルの正解率: {dt_accuracy:.2f}\")\n","\n","# ランダムフォレストモデルを作成して訓練\n","rf_model = RandomForestClassifier()\n","rf_model.fit(X_train, y_train)\n","\n","# ランダムフォレストモデルの予測値を取得し、正解率を算出\n","rf_pred = rf_model.predict(X_test)\n","rf_accuracy = accuracy_score(y_test, rf_pred)\n","print(f\"ランダムフォレストモデルの正解率: {rf_accuracy:.2f}\")"],"metadata":{"id":"GIVgFAko5Him"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## 実践8-3\n","* がんデータを読み込んでください。\n","```\n","from sklearn.datasets import load_breast_cancer\n","cancer = load_breast_cancer()\n","```\n","* iris同様、決定木とランダムフォレストのモデルを用いて推論してください。\n","* 正解率(accuracy)を算出してください\n","* (応用) 適合率(precision), 再現率(recall), F1値を算出してください。（ヒント: \"講義資料の予測モデルの評価\"にI/F記載あり）\n"],"metadata":{"id":"fu9w33JMu0Xh"}},{"cell_type":"code","source":["from sklearn.datasets import load_breast_cancer\n","from sklearn.model_selection import train_test_split\n","from sklearn.tree import DecisionTreeClassifier\n","from sklearn.ensemble import RandomForestClassifier\n","from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n","\n","# がんデータロード\n","cancer = load_breast_cancer()\n","X = cancer.data\n","y = cancer.target\n","\n","# 学習データとテストデータに分割\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n","\n","# 決定木モデル\n","dt = DecisionTreeClassifier()\n","dt.fit(X_train, y_train)\n","y_pred_dt = dt.predict(X_test)\n","\n","# ランダムフォレストモデル\n","rf = RandomForestClassifier(n_estimators=100)\n","rf.fit(X_train, y_train)\n","y_pred_rf = rf.predict(X_test)\n","\n","# 評価指標計算 (決定木)\n","acc_dt = accuracy_score(y_test, y_pred_dt)\n","prec_dt = precision_score(y_test, y_pred_dt)\n","rec_dt = recall_score(y_test, y_pred_dt)\n","f1_dt = f1_score(y_test, y_pred_dt)\n","\n","# 評価指標計算 (ランダムフォレスト)\n","acc_rf = accuracy_score(y_test, y_pred_rf)\n","prec_rf = precision_score(y_test, y_pred_rf)\n","rec_rf = recall_score(y_test, y_pred_rf)\n","f1_rf = f1_score(y_test, y_pred_rf)\n","\n","# 結果出力\n","print('決定木モデル 正解率(accuracy):', acc_dt)\n","print('決定木モデル 適合率(precision):', prec_dt)\n","print('決定木モデル 再現率(recall):', rec_dt)\n","print('決定木モデル F1値:', f1_dt)\n","print('-'*30)\n","print('ランダムフォレストモデル 正解率(accuracy):', acc_rf)\n","print('ランダムフォレストモデル 適合率(precision):', prec_rf)\n","print('ランダムフォレストモデル 再現率(recall):', rec_rf)\n","print('ランダムフォレストモデル F1値:', f1_rf)"],"metadata":{"id":"O5rfbbCKu0wE"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# 実践9"],"metadata":{"id":"QNxf0HKeylWL"}},{"cell_type":"markdown","source":["## 実践9-1\n","* 以下のサンプルデータを作成してください。\n","```\n","data = {\n","    'name': ['Alice', 'Bob', 'Charlie', 'David', 'Eve', 'Frank', 'Grace', 'Heidi', 'Ivan', 'Julia', 'Kevin', 'Liam', 'Maya', 'Noah'],\n","    'age': [25, 30, 35, 40, 28, None, 32, 22, 45, 37, 51, 24, 29, None],\n","    'city': ['New York', 'London', 'Paris', 'Tokyo', 'Berlin', 'Moscow', 'Sydney', 'New York', 'London', 'Paris', 'Tokyo', 'Berlin', 'Moscow', 'Sydney'],\n","    'score': [85, 92, 88, 75, 90, 82, None, 79, 95, 68, 81, 72, 87, 63]\n","}\n","```\n","* 以下の前処理（含む可視化）を実施してください\n","    * サンプルデータ(name, age, city, score)を作成\n","    * データの確認(先頭行、データ型など)\n","    * 欠損値の確認と除外\n","    * 年齢の外れ値(25歳未満、45歳超)の除外\n","    * スコアの平均を計算し、新しい列に追加（あくまで新しい列作成の演習: 実際には利用しない）\n","    * 都市ごとのスコアの平均を計算\n","    * 年齢とスコアの関係を散布図で可視化\n","    * 都市ごとのスコアの分布を棒グラフで可視化\n","\n","\n"],"metadata":{"id":"1ytJzbKNyz0J"}},{"cell_type":"code","source":["import pandas as pd\n","import matplotlib.pyplot as plt\n","\n","# サンプルデータを作成\n","data = {\n","    'name': ['Alice', 'Bob', 'Charlie', 'David', 'Eve', 'Frank', 'Grace', 'Heidi', 'Ivan', 'Julia', 'Kevin', 'Liam', 'Maya', 'Noah'],\n","    'age': [25, 30, 35, 40, 28, None, 32, 22, 45, 37, 51, 24, 29, None],\n","    'city': ['New York', 'London', 'Paris', 'Tokyo', 'Berlin', 'Moscow', 'Sydney', 'New York', 'London', 'Paris', 'Tokyo', 'Berlin', 'Moscow', 'Sydney'],\n","    'score': [85, 92, 88, 75, 90, 82, None, 79, 95, 68, 81, 72, 87, 63]\n","}\n","df = pd.DataFrame(data)\n","\n","# データの確認\n","print('先頭5行:')\n","display(df.head())\n","print('\\nデータ型:')\n","display(df.dtypes)\n","\n","# 欠損値の確認と除外\n","print(f'\\n欠損値数: {df.isna().sum()}')\n","df = df.dropna()\n","\n","# 年齢の外れ値の除外\n","df = df[(df['age'] >= 25) & (df['age'] <= 45)]\n","\n","# スコアの平均を計算し、新しい列に追加\n","df['avg_score'] = df['score'].mean()\n","\n","# 都市ごとのスコアの平均を計算\n","print('\\n都市ごとのスコアの平均:')\n","display(df.groupby('city')['score'].mean())\n","\n","# 年齢とスコアの関係を散布図で可視化\n","plt.figure(figsize=(8, 6))\n","plt.scatter(df['age'], df['score'])\n","plt.title('Age vs. Score')\n","plt.xlabel('Age')\n","plt.ylabel('Score')\n","plt.show()\n","\n","# 都市ごとのスコアの分布を棒グラフで可視化\n","plt.figure(figsize=(8, 6))\n","df.groupby('city')['score'].mean().plot(kind='bar')\n","plt.title('Average Score by City')\n","plt.xlabel('City')\n","plt.ylabel('Average Score')\n","plt.show()"],"metadata":{"id":"-1DYygeH58Ug"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## 実践9-2(応用)\n","* seabornのtipsデータをロードしてください。\n","```\n","import seaborn as sns\n","# データセットのロード\n","tips = sns.load_dataset(\"tips\")\n","```\n","    * total_bill: 会計総額\n","    * tip: 支払われたチップ\n","    * sex: 支払い主の性別\n","    * smoker: 喫煙者かどうか\n","    * day: 来店日(Sun, Sat, Thur, Fri)\n","    * time: 時間帯(Dinner, Lunch)\n","    * size: 同伴者の人数\n","* このデータの特徴を確認して説明してください。(基礎分析)\n","* 外れ値がないか確認してください。\n","* チップの割合(%)を新しい列として作成してください\n","* \"smoker\"カラムを機械学習に適した値(1 or 0) に変換してください。\n","* dayカラムの型を確認し、category型に変換してください。(ヒント: astypeを利用する)\n","* チップの割合の分布を可視化してください。その際、可能ならseabornを利用してください。\n","* sns.catplot(棒グラフ)を使ってチップの割合以外を可視化してください。\n","* 何かしら不要と思われる列を削除してください。\n","\n"],"metadata":{"id":"1SQw9k26zQVi"}},{"cell_type":"code","source":["import seaborn as sns\n","import matplotlib.pyplot as plt\n","\n","# データセットのロード\n","tips = sns.load_dataset(\"tips\")\n","\n","# データの基礎分析\n","print(tips.head())  # 先頭5行を表示\n","print(tips.describe())  # 量的データの基本統計量（外れ値確認含む）\n","print(tips.info())  # データの概要(型,欠損値数など)\n","\n","# チップの割合(%)を新しい列として追加\n","tips[\"tips_percentage\"] = tips[\"tip\"] / tips[\"total_bill\"] * 100\n","\n","# \"smoker\"カラムを0,1に変換\n","tips[\"smoker\"] = tips[\"smoker\"].map({\"Yes\": 1, \"No\": 0})\n","\n","# dayのデータ型をcategory型に変換\n","tips[\"day\"] = tips[\"day\"].astype(\"category\")\n","\n","# チップの割合の分布を可視化\n","sns.displot(tips[\"tips_percentage\"], kde=True)\n","#plt.show()\n","\n","# 時間帯別の支払い額を可視化\n","sns.catplot(x=\"time\", y=\"total_bill\", kind=\"bar\", data=tips)\n","plt.show()\n","\n","# 不要な列を削除\n","tips = tips.drop(\"sex\", axis=1)\n"],"metadata":{"id":"964hAFgf8n5Z"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# 実践11"],"metadata":{"id":"kc-_NgcizXyf"}},{"cell_type":"markdown","source":["## 実践11-1:\n","* 以下のデータを読み込んでください。\n","```\n","from sklearn.datasets import load_wine\n","data = load_wine(as_frame=True)\n","X = data.data\n","y = data.target\n","```\n","* 各変数の特徴（含む欠損・外れ値）を可視化や数値計算等で確認してください。必要に応じて加工してください。\n","* 標準化をしてください。\n","* （応用：可能であれば）多重共線性を確認し、必要に応じて変数を削除してください。その結果でモデルを変更してください。ヒント: statsmodelのvariance_inflation_factorを用いてVIFを算出する。\n","* 回帰モデルを作成してください。\n","* 推論結果を確認してください。その際、以下の確認をしてください。\n","    * 決定係数 (R^2)\n","    * RMSE\n"],"metadata":{"id":"4vDwsRMWz1z-"}},{"cell_type":"code","source":["import pandas as pd\n","import numpy as np\n","from sklearn.preprocessing import StandardScaler\n","from sklearn.linear_model import LinearRegression\n","from sklearn.metrics import r2_score, mean_squared_error\n","import statsmodels.api as sm\n","from statsmodels.stats.outliers_influence import variance_inflation_factor\n","\n","# データの読み込み\n","from sklearn.datasets import load_wine\n","data = load_wine(as_frame=True)\n","X = data.data\n","y = data.target\n","\n","# 変数の特徴確認\n","print('変数の概要:')\n","print(X.describe())\n","\n","# 欠損値の確認\n","print(f'\\n欠損値数: {X.isna().sum().sum()}')\n","\n","# 外れ値の確認\n","outlier_threshold = 3  # 外れ値のしきい値 (3 * 標準偏差)\n","for col in X.columns:\n","    mean = X[col].mean()\n","    std = X[col].std()\n","    lower_bound = mean - outlier_threshold * std\n","    upper_bound = mean + outlier_threshold * std\n","    outliers = X[(X[col] < lower_bound) | (X[col] > upper_bound)]\n","    if not outliers.empty:\n","        print(f\"変数 {col} の外れ値の数: {len(outliers)}\")\n","\n","# 標準化\n","scaler = StandardScaler()\n","X_scaled = scaler.fit_transform(X)\n","\n","# VIF\n","## 分散膨張係数(VIF)による多重共線性の確認\n","vif = pd.DataFrame()\n","vif[\"VIF Factor\"] = [variance_inflation_factor(X_scaled, i) for i in range(X_scaled.shape[1])]\n","vif[\"features\"] = X.columns\n","vif = vif.sort_values(by=\"VIF Factor\", ascending=False)\n","print(f'\\n分散膨張係数(VIF):')\n","print(vif)\n","\n","## 多重共線性が高い変数の削除\n","high_vif = vif[vif[\"VIF Factor\"] > 10].features.tolist()\n","print(f'\\n多重共線性が高い変数: {high_vif}')\n","X_reduced_scaled = X_scaled[:, [i for i in range(X_scaled.shape[1]) if X.columns[i] not in high_vif]]\n","\n","# 回帰モデルの作成と学習\n","model = LinearRegression()\n","model.fit(X_reduced_scaled, y)\n","\n","# 推論結果\n","y_pred = model.predict(X_reduced_scaled)\n","r2 = r2_score(y, y_pred)\n","rmse = np.sqrt(mean_squared_error(y, y_pred))\n","print(f'\\n決定係数 (R^2): {r2:.2f}')\n","print(f'RMSE: {rmse:.2f}')"],"metadata":{"id":"AZdUd_J1-vCz"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## 実践11-2 (応用)\n","* 標準化以外の代表的な変換について調べ、標準化(StandardScaler)との違いを記載してください。\n","* 上記実践1と同じ問題で別の変換手法を用いて評価してください。"],"metadata":{"id":"HDeKvwZ50JFw"}},{"cell_type":"markdown","source":["### 標準化以外の代表的な変換手法\n","\n","* 正規化 (Normalization):\n","    \n","    - 目的: 変数の値を0から1の範囲にスケーリングする\n","    - 種類:\n","        - Min-Max正規化: 変数の値を最小値0、最大値1の範囲にスケーリング\n","        - L1正規化: 各サンプルの絶対値の合計を1にするようスケーリング\n","        - L2正規化: 各サンプルのベクトルノルムを1にするようスケーリング\n","\n","    - 利点: 外れ値の影響を抑える、変数間のスケールを揃える\n","    - 欠点: 外れ値の影響を完全に除去できない、変数間の分散情報が失われる\n","\n","* 対数変換 (Log Transformation):\n","\n","    - 目的: 右裾の長い分布を対数をとることで正規化する\n","    - 手法: 変数の値に対数関数(通常は自然対数)を適用する\n","    - 利点: 右裾の長い分布を正規分布に近付ける、大きな値の影響を抑える\n","    - 欠点: 負の値や0の値が存在する場合に適用できない\n","\n","* ボックスコックス変換 (Box-Cox Transformation):\n","\n","    - 目的: 正規分布に近い分布に変換する\n","    - 手法: 変数の値に対して最適なパラメータλでべき乗変換を行う\n","    - 利点: より一般的な変換で正規性を高められる\n","    - 欠点: 変換パラメータλの選択が難しい、負の値が存在すると適用できない\n","        - 参考: 負の値に適用するにはYeo-Johnsonという手法を用いるとよい\n","\n","* カテゴリ変数のエンコーディング:\n","\n","    - 目的: カテゴリ変数を数値に変換する\n","    - 手法: ラベルエンコーディング、ワンホットエンコーディングなど\n","    - 利点: 機械学習モデルで扱えるようになる\n","    - 欠点: カテゴリ間の関係が失われる(ラベルエンコーディング)、次元が増える(ワンホットエンコーディング)\n","\n","* 標準化(StandardScaler)との違い\n","\n","    - 標準化は、変数の平均を0、分散を1にスケーリングする手法です。これにより、変数間のスケールが揃い、外れ値の影響が軽減されます。標準化は、線形モデルや距離ベースのモデルで有効です。\n","    - 一方、他の変換手法は以下のような特徴があります:\n","\n","        - 正規化: 値の範囲を0から1にスケーリングするが、分散情報は失われる\n","        - 対数変換: 右裾の長い分布を正規分布に近付けるが、負値や0がある場合は適用できない\n","        - ボックスコックス変換: より一般的な変換で正規性を高められるが、パラメータ選択が難しい\n","        - カテゴリ変数のエンコーディング: カテゴリ変数を数値化するが、カテゴリ間の関係が失われる可能性がある\n","\n","データの分布の形状や変数の種類に応じて、適切な変換手法を選択する必要があります。"],"metadata":{"id":"nH7Z4k9qKoV6"}},{"cell_type":"code","source":["import pandas as pd\n","import numpy as np\n","from sklearn.preprocessing import StandardScaler\n","from sklearn.linear_model import LinearRegression\n","from sklearn.metrics import r2_score, mean_squared_error\n","import statsmodels.api as sm\n","from statsmodels.stats.outliers_influence import variance_inflation_factor\n","\n","# データの読み込み\n","from sklearn.datasets import load_wine\n","data = load_wine(as_frame=True)\n","X = data.data\n","y = data.target\n","\n","# 対数変換\n","X_log = np.log1p(X)  # 0の値がある場合に対応するため、np.log1p(x) = log(1 + x)を使用\n","\n","# 変数の特徴確認\n","print('変数の概要(対数変換後):')\n","print(X_log.describe())\n","\n","# 欠損値の確認\n","print(f'\\n欠損値数: {X_log.isna().sum().sum()}')\n","\n","# 分散膨張係数(VIF)による多重共線性の確認\n","vif = pd.DataFrame()\n","vif[\"VIF Factor\"] = [variance_inflation_factor(X_log.values, i) for i in range(X_log.shape[1])]\n","vif[\"features\"] = X_log.columns\n","vif = vif.sort_values(by=\"VIF Factor\", ascending=False)\n","print(f'\\n分散膨張係数(VIF):')\n","print(vif)\n","\n","# 多重共線性が高い変数の削除(ただし今の変換だとほぼ全て高くなってしまうので、VIFの値を30へ変更)\n","high_vif = vif[vif[\"VIF Factor\"] > 30].features.tolist()\n","print(f'\\n多重共線性が高い変数: {high_vif}')\n","X_reduced_log = X_log.drop(high_vif, axis=1)\n","\n","# 標準化\n","scaler = StandardScaler()\n","X_reduced_scaled = scaler.fit_transform(X_reduced_log)\n","\n","# 回帰モデルの作成と学習\n","model = LinearRegression()\n","model.fit(X_reduced_scaled, y)\n","\n","# 推論結果\n","y_pred = model.predict(X_reduced_scaled)\n","r2 = r2_score(y, y_pred)\n","rmse = np.sqrt(mean_squared_error(y, y_pred))\n","print(f'\\n決定係数 (R^2): {r2:.2f}')\n","print(f'RMSE: {rmse:.2f}')"],"metadata":{"id":"bm-YBTr7L4oi"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## 実践11-3(応用)\n","* 11-1と同じデータに対してRidge, Lasso, ElasticNetを適用して評価してください。\n","* 多重共線性はRidge/Lasso/ElasticNet側で弱めるので明示的な記載は不要です。\n"],"metadata":{"id":"O8jwzB178UWJ"}},{"cell_type":"code","source":["import pandas as pd\n","import numpy as np\n","from sklearn.preprocessing import StandardScaler\n","from sklearn.linear_model import Ridge, Lasso, ElasticNet\n","from sklearn.metrics import r2_score, mean_squared_error\n","import statsmodels.api as sm\n","from statsmodels.stats.outliers_influence import variance_inflation_factor\n","\n","# データの読み込み\n","from sklearn.datasets import load_wine\n","data = load_wine(as_frame=True)\n","X = data.data\n","y = data.target\n","\n","# 変数の特徴確認\n","print('変数の概要:')\n","print(X.describe())\n","\n","# 標準化\n","scaler = StandardScaler()\n","X_scaled = scaler.fit_transform(X)\n","\n","# Ridge回帰\n","ridge = Ridge(alpha=1.0)\n","ridge.fit(X_scaled, y)\n","y_pred_ridge = ridge.predict(X_scaled)\n","r2_ridge = r2_score(y, y_pred_ridge)\n","rmse_ridge = np.sqrt(mean_squared_error(y, y_pred_ridge))\n","print(f'\\nRidge回帰 - 決定係数 (R^2): {r2_ridge:.2f}, RMSE: {rmse_ridge:.2f}')\n","\n","# Lasso回帰\n","lasso = Lasso(alpha=1.0)\n","lasso.fit(X_scaled, y)\n","y_pred_lasso = lasso.predict(X_scaled)\n","r2_lasso = r2_score(y, y_pred_lasso)\n","rmse_lasso = np.sqrt(mean_squared_error(y, y_pred_lasso))\n","print(f'\\nLasso回帰 - 決定係数 (R^2): {r2_lasso:.2f}, RMSE: {rmse_lasso:.2f}')\n","\n","# ElasticNet回帰\n","elasticnet = ElasticNet(alpha=1.0, l1_ratio=0.5)\n","elasticnet.fit(X_scaled, y)\n","y_pred_elasticnet = elasticnet.predict(X_scaled)\n","r2_elasticnet = r2_score(y, y_pred_elasticnet)\n","rmse_elasticnet = np.sqrt(mean_squared_error(y, y_pred_elasticnet))\n","print(f'\\nElasticNet回帰 - 決定係数 (R^2): {r2_elasticnet:.2f}, RMSE: {rmse_elasticnet:.2f}')"],"metadata":{"id":"kNU8h7trMg9p"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# 実践12"],"metadata":{"id":"9rZSEERM2Tuh"}},{"cell_type":"markdown","source":["## 実践12-1 (pandasの復習)\n","* https://terakoya.sejuku.net/programs/110/chapters/1379 の「7. 予測モデルの評価」で実行したload_breast_cancer()のclassification_reportをdataframeに格納してください。行名・列名もわかるようにしてください。その後csvとして保存してください。\n","* 実務でもdataframeに格納してcsvへ保存するケースはあります。"],"metadata":{"id":"sX9lQh1R2ZCX"}},{"cell_type":"code","source":["import pandas as pd\n","from sklearn.datasets import load_breast_cancer\n","dataset = load_breast_cancer()\n","df = pd.DataFrame(dataset.data, columns=dataset.feature_names)\n","df['class'] = dataset.target\n","\n","X = df.drop(columns=['class']).to_numpy()\n","y = df['class'].to_numpy()\n","\n","from sklearn.model_selection import train_test_split\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0)\n","\n","\n","\n","from sklearn.tree import DecisionTreeClassifier\n","model = DecisionTreeClassifier(random_state=0)\n","model.fit(X_train, y_train)\n","y_pred = model.predict(X_test)\n","from sklearn.metrics import classification_report\n","\n","# 分類レポートの生成・DataFrameに変換\n","report = classification_report(y_test, y_pred, output_dict=True)\n","report_df = pd.DataFrame(report).transpose()\n","\n","# CSVとして保存\n","report_df.to_csv('classification_report.csv', index=True)\n"],"metadata":{"id":"yAS-uPcMTQ5i"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## 実践12-2(matplotlib, seabornの復習 & 一部発展)\n","* load_breast_cancerを講義テキストでは\n","```\n","df.hist(figsize=(20, 15), bins=30)\n","```\n","として実施しました。\n","* これをseabornのstripplotを使って同様に表示してください。その際、全特徴量を1つのfigとして表示し、図を保存してください。\n","* ポイント: subplotsを用いること & for文を使ってstripplotをおこなう。\n","* 参考) 実務でもこのように全特徴量に対して一斉の可視化を行うケース(pandasの可視化だけでは対応できないケース)はあります。一度ロジックを組み立てておけば汎用的に使えるので覚えておくと便利です。\n","* 更に発展させると、この処理のベースを関数/クラスのメソッド化しておき、dataframeを渡すだけにしておくと更に利便性が高まります。\n"],"metadata":{"id":"J1gVk_A04w9T"}},{"cell_type":"code","source":["import seaborn as sns\n","import matplotlib.pyplot as plt\n","import pandas as pd\n","from sklearn.datasets import load_breast_cancer\n","\n","# データの読み込み\n","data = load_breast_cancer()\n","df = pd.DataFrame(data.data, columns=data.feature_names)\n","df['target'] = data.target\n","\n","# サブプロットの設定\n","fig, axes = plt.subplots(nrows=5, ncols=6, figsize=(20, 15), constrained_layout=True)\n","\n","# axesをフラット化してループで扱いやすくする\n","axes = axes.flatten()\n","\n","# 各特徴量に対してstripplotを描画\n","for i, feature in enumerate(df.columns[:-1]):\n","    sns.stripplot(ax=axes[i], x='target', y=feature, data=df, palette='Set2')\n","    axes[i].set_title(feature)\n","    axes[i].set_xlabel('Target')\n","    axes[i].set_ylabel('Value')\n","\n","# 余ったサブプロットを削除\n","for j in range(i+1, len(axes)):\n","    fig.delaxes(axes[j])\n","\n","# 画像の保存\n","plt.savefig('/mnt/data/breast_cancer_stripplot.png')\n","plt.show()\n"],"metadata":{"id":"3fg8KuJJTyyF"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 上記を関数化したバージョン\n","\n","import seaborn as sns\n","import matplotlib.pyplot as plt\n","import pandas as pd\n","import math\n","\n","def plot_dynamic_stripplot(df, target_column, output_path=None, figsize=(20, 15)):\n","    \"\"\"\n","    データフレームの各特徴量に対してstripplotを動的に描画する関数\n","\n","    Parameters:\n","    df (pandas.DataFrame): 入力データフレーム\n","    target_column (str): ターゲット列の名前\n","    output_path (str, optional): 出力画像のパス。指定しない場合は画像を保存しない\n","    figsize (tuple, optional): 図のサイズ。デフォルトは(20, 15)\n","    \"\"\"\n","    # ターゲット列を除いた特徴量の数を取得\n","    n_features = len(df.columns) - 1\n","\n","    # 行数と列数を計算（列数を優先的に調整）\n","    n_cols = min(6, n_features)  # 最大列数を6に制限\n","    n_rows = math.ceil(n_features / n_cols)\n","\n","    # サブプロットの設定\n","    fig, axes = plt.subplots(nrows=n_rows, ncols=n_cols, figsize=figsize, constrained_layout=True)\n","\n","    # axesをフラット化してループで扱いやすくする\n","    axes = axes.flatten() if n_rows > 1 or n_cols > 1 else [axes]\n","\n","    # 各特徴量に対してstripplotを描画\n","    for i, feature in enumerate(df.columns.drop(target_column)):\n","        sns.stripplot(ax=axes[i], x=target_column, y=feature, data=df, hue=target_column, palette='Set2', legend=False)\n","        axes[i].set_title(feature)\n","        axes[i].set_xlabel('Target')\n","        axes[i].set_ylabel('Value')\n","\n","    # 余ったサブプロットを削除\n","    for j in range(i+1, len(axes)):\n","        fig.delaxes(axes[j])\n","\n","    # 画像の保存（パスが指定されている場合）\n","    if output_path:\n","        plt.savefig(output_path)\n","\n","    plt.show()\n","\n","# 使用例\n","if __name__ == \"__main__\":\n","    from sklearn.datasets import load_breast_cancer\n","\n","    # データの読み込み\n","    data = load_breast_cancer()\n","    df = pd.DataFrame(data.data, columns=data.feature_names)\n","    df['target'] = data.target\n","\n","    # 関数の呼び出し\n","    plot_dynamic_stripplot(df, 'target')"],"metadata":{"id":"RloogSe_qiro"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## 実践12-3\n","- https://terakoya.sejuku.net/programs/110/chapters/1379 のload_breast_cancer()での「7. 予測モデルの評価」の流れで混同行列を作成し表示してください。(classification_reportの元となる行列で実務でも利用します)\n","- 混同行列をdataframe(index, column名付与)して表示・保存してください。"],"metadata":{"id":"3Zcmj7JO4-T0"}},{"cell_type":"code","source":["from sklearn.datasets import load_breast_cancer\n","from sklearn.model_selection import train_test_split\n","from sklearn.ensemble import RandomForestClassifier\n","from sklearn.metrics import confusion_matrix\n","import pandas as pd\n","import seaborn as sns\n","import matplotlib.pyplot as plt\n","\n","# データの読み込み\n","data = load_breast_cancer()\n","X, y = data.data, data.target\n","\n","# データの分割\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n","\n","# モデルの訓練\n","model = RandomForestClassifier(random_state=42)\n","model.fit(X_train, y_train)\n","\n","# テストデータに対する予測\n","y_pred = model.predict(X_test)\n","\n","# 混同行列の作成\n","conf_matrix = confusion_matrix(y_test, y_pred)\n","\n","# 混同行列をDataFrameに変換\n","conf_matrix_df = pd.DataFrame(conf_matrix, index=data.target_names, columns=data.target_names)\n","\n","# 混同行列の表示\n","print(\"Confusion Matrix:\")\n","print(conf_matrix_df)\n","\n","# 混同行列の保存\n","conf_matrix_df.to_csv('/mnt/data/breast_cancer_confusion_matrix.csv')\n","\n","# ヒートマップのプロット\n","plt.figure(figsize=(8, 6))\n","sns.heatmap(conf_matrix_df, annot=True, fmt='d', cmap='Blues')\n","plt.title('Confusion Matrix')\n","plt.xlabel('Predicted Labels')\n","plt.ylabel('True Labels')\n","\n","# ヒートマップの保存\n","plt.savefig('/mnt/data/breast_cancer_confusion_matrix.png')\n","plt.show()\n"],"metadata":{"id":"qMsahpZKUKd-"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## 実践12-4(マルチクラス分類)\n","* irisデータを取得してください。\n","```\n","from sklearn.datasets import load_iris\n","iris = load_iris()\n","X = iris.data\n","y = iris.target\n","```\n","* irisデータに対して学習・推論を実施してください。モデルは任意で構いませんが、木構造以外のアルゴリズムの場合は標準化を視野に入れてください(厳密にはデータによっては標準化が不要な場合もありますが、基本は実施すべきです)。木構造の場合はexport_text等で条件分岐を出力してください。\n","* 混同行列や評価指標を算出してください。\n"],"metadata":{"id":"2gi71jF15LUb"}},{"cell_type":"code","source":["from sklearn.datasets import load_iris\n","from sklearn.model_selection import train_test_split\n","from sklearn.preprocessing import StandardScaler\n","from sklearn.svm import SVC\n","from sklearn.metrics import confusion_matrix, classification_report\n","import pandas as pd\n","import seaborn as sns\n","import matplotlib.pyplot as plt\n","\n","# Irisデータの読み込み\n","iris = load_iris()\n","X, y = iris.data, iris.target\n","\n","# データの分割\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42, stratify=y)\n","\n","# データの標準化\n","scaler = StandardScaler()\n","X_train_scaled = scaler.fit_transform(X_train)\n","X_test_scaled = scaler.transform(X_test)\n","\n","# モデルの訓練\n","model = SVC(kernel='linear', random_state=42)\n","model.fit(X_train_scaled, y_train)\n","\n","# テストデータに対する予測\n","y_pred = model.predict(X_test_scaled)\n","\n","# 混同行列の作成\n","conf_matrix = confusion_matrix(y_test, y_pred)\n","\n","# 混同行列をDataFrameに変換\n","conf_matrix_df = pd.DataFrame(conf_matrix, index=iris.target_names, columns=iris.target_names)\n","\n","# 混同行列の表示\n","print(\"Confusion Matrix:\")\n","print(conf_matrix_df)\n","\n","# 混同行列の保存\n","conf_matrix_df.to_csv('/mnt/data/iris_confusion_matrix.csv')\n","\n","# ヒートマップのプロット\n","plt.figure(figsize=(8, 6))\n","sns.heatmap(conf_matrix_df, annot=True, fmt='d', cmap='Blues')\n","plt.title('Confusion Matrix - Iris Data')\n","plt.xlabel('Predicted Labels')\n","plt.ylabel('True Labels')\n","\n","# ヒートマップの保存\n","plt.savefig('/mnt/data/iris_confusion_matrix.png')\n","plt.show()\n","\n","# 評価指標の算出\n","report = classification_report(y_test, y_pred, target_names=iris.target_names)\n","print(\"\\nClassification Report:\")\n","print(report)\n","\n","# 評価指標をDataFrameに変換\n","report_df = pd.DataFrame(classification_report(y_test, y_pred, target_names=iris.target_names, output_dict=True)).transpose()\n","\n","# 評価指標の保存\n","report_df.to_csv('/mnt/data/iris_classification_report.csv')\n","\n","# 評価指標の表示\n","print(\"\\nClassification Report DataFrame:\")\n","print(report_df)\n"],"metadata":{"id":"a3NqZBr9UZ7R"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# 実践13"],"metadata":{"id":"2V6RbZQ85WSs"}},{"cell_type":"markdown","source":["## 実践13-1\n","* K-Meansクラスタリングによる品種の分類\n","    * irisデータセットをロードする\n","    * 説明変数(4つの特徴量)を抽出する\n","    * K-Means法によりクラスタリングを行う\n","    * エルボー法で最適なクラスタ数を決定する\n","    * クラスタリング結果と実際の品種ラベルを比較し、正解率を計算する"],"metadata":{"id":"duShAMiu5Z2u"}},{"cell_type":"code","source":["import numpy as np\n","import pandas as pd\n","from sklearn.datasets import load_iris\n","from sklearn.cluster import KMeans\n","from sklearn.metrics import accuracy_score\n","import matplotlib.pyplot as plt\n","\n","# irisデータセットをロードする\n","iris = load_iris()\n","X = iris.data\n","y = iris.target\n","\n","# エルボー法で最適なクラスタ数を決定する\n","inertias = []\n","for k in range(1, 11):\n","    kmeans = KMeans(n_clusters=k, random_state=42)\n","    kmeans.fit(X)\n","    inertias.append(kmeans.inertia_)\n","\n","# エルボーカーブをプロット\n","plt.plot(range(1, 11), inertias, marker='o')\n","plt.xlabel('Number of clusters (k)')\n","plt.ylabel('Inertia')\n","plt.title('Elbow Method for Optimal k')\n","plt.show()\n","\n","# 最適なクラスタ数を選択 (この場合、視覚的に3を選択)\n","optimal_k = 3\n","\n","# K-Means法によりクラスタリングを行う\n","kmeans = KMeans(n_clusters=optimal_k, random_state=42)\n","cluster_labels = kmeans.fit_predict(X)\n","\n","# クラスタリング結果と実際の品種ラベルを比較し、正解率を計算する\n","# クラスタラベルと実際のラベルのマッピングを求める\n","label_mapping = {}\n","for i in range(optimal_k):\n","    mask = (cluster_labels == i)\n","    label_mapping[i] = np.argmax(np.bincount(y[mask]))\n","\n","# マッピングを使用して予測ラベルを変換\n","predicted_labels = np.array([label_mapping[label] for label in cluster_labels])\n","\n","# 正解率を計算\n","accuracy = accuracy_score(y, predicted_labels)\n","print(f\"Clustering Accuracy: {accuracy:.2f}\")\n","\n","# 結果の可視化 (最初の2つの特徴量を使用)\n","plt.scatter(X[:, 0], X[:, 1], c=cluster_labels, cmap='viridis')\n","plt.xlabel(iris.feature_names[0])\n","plt.ylabel(iris.feature_names[1])\n","plt.title('K-Means Clustering Result')\n","plt.show()"],"metadata":{"id":"L3cu-1kFIx_E"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## 実践13-2\n","* K-Means以外のクラスタリング手法による品種の分類\n","    * 13-1をK-Means以外の手法で実施する(階層的クラスタリング・DBSCAN・etc)\n","    * クラスタリング結果と実際の品種ラベルを比較し、正解率を計算する"],"metadata":{"id":"TEMGwAdA6IZy"}},{"cell_type":"code","source":["import numpy as np\n","import pandas as pd\n","from sklearn.datasets import load_iris\n","from sklearn.cluster import AgglomerativeClustering, DBSCAN\n","from sklearn.metrics import accuracy_score, adjusted_rand_score\n","from sklearn.preprocessing import StandardScaler\n","import matplotlib.pyplot as plt\n","\n","# irisデータセットをロードする\n","iris = load_iris()\n","X = iris.data\n","y = iris.target\n","\n","# データの標準化\n","scaler = StandardScaler()\n","X_scaled = scaler.fit_transform(X)\n","\n","# クラスタリング手法とそのパラメータを定義\n","clustering_methods = [\n","    (\"Hierarchical\", AgglomerativeClustering(n_clusters=3)),\n","    (\"DBSCAN\", DBSCAN(eps=0.5, min_samples=5))\n","]\n","\n","# 結果を格納するリスト\n","results = []\n","\n","# 各クラスタリング手法を実行\n","for name, method in clustering_methods:\n","    # クラスタリングを実行\n","    cluster_labels = method.fit_predict(X_scaled)\n","\n","    # クラスタ数を取得 (DBSCANの場合、-1はノイズポイントを表す)\n","    n_clusters = len(set(cluster_labels)) - (1 if -1 in cluster_labels else 0)\n","\n","    # クラスタリング結果と実際の品種ラベルを比較し、正解率を計算する\n","    # クラスタラベルと実際のラベルのマッピングを求める\n","    label_mapping = {}\n","    for i in range(n_clusters):\n","        mask = (cluster_labels == i)\n","        if np.sum(mask) > 0:  # クラスタが空でない場合のみマッピング\n","            label_mapping[i] = np.argmax(np.bincount(y[mask]))\n","\n","    # マッピングを使用して予測ラベルを変換 (DBSCANのノイズポイントは-1のまま)\n","    predicted_labels = np.array([label_mapping.get(label, -1) for label in cluster_labels])\n","\n","    # 正解率を計算 (ノイズポイントは除外)\n","    mask = (predicted_labels != -1)\n","    accuracy = accuracy_score(y[mask], predicted_labels[mask])\n","\n","    # Adjusted Rand Indexを計算\n","    ari = adjusted_rand_score(y, cluster_labels)\n","\n","    results.append({\n","        \"Method\": name,\n","        \"Accuracy\": accuracy,\n","        \"ARI\": ari,\n","        \"n_clusters\": n_clusters,\n","        \"labels\": cluster_labels\n","    })\n","\n","    print(f\"{name} Clustering:\")\n","    print(f\"Number of clusters: {n_clusters}\")\n","    print(f\"Accuracy: {accuracy:.2f}\")\n","    print(f\"Adjusted Rand Index: {ari:.2f}\")\n","    print()\n","\n","# 結果の可視化 (最初の2つの特徴量を使用)\n","fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n","for ax, result in zip(axes, results):\n","    scatter = ax.scatter(X[:, 0], X[:, 1], c=result[\"labels\"], cmap='viridis')\n","    ax.set_xlabel(iris.feature_names[0])\n","    ax.set_ylabel(iris.feature_names[1])\n","    ax.set_title(f'{result[\"Method\"]} Clustering Result')\n","    plt.colorbar(scatter, ax=ax)\n","\n","plt.tight_layout()\n","plt.show()"],"metadata":{"id":"FVx8X8dtJkBw"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## 実践13-3\n","\n","\n","\n","```\n","from sklearn.datasets import load_wine\n","wine = load_wine()\n","X, y = wine.data, wine.target\n","```\n","\n","上記のデータを使ってクラスタリングを実施してください。\n","特徴量のスケーリング等に注意して実施してください。\n","本データは実際には正解データ(ラベル数)が付与されています。\n","クラスタリングである程度分類できるか正解率を計算してください。"],"metadata":{"id":"R3_gjQ7L7Szv"}},{"cell_type":"code","source":["import numpy as np\n","import pandas as pd\n","from sklearn.datasets import load_wine\n","from sklearn.preprocessing import StandardScaler\n","from sklearn.cluster import KMeans\n","from sklearn.metrics import accuracy_score, adjusted_rand_score\n","from sklearn.decomposition import PCA\n","import matplotlib.pyplot as plt\n","\n","# ワインデータセットをロード\n","wine = load_wine()\n","X, y = wine.data, wine.target\n","\n","# データの標準化\n","scaler = StandardScaler()\n","X_scaled = scaler.fit_transform(X)\n","\n","# エルボー法で最適なクラスタ数を決定\n","inertias = []\n","for k in range(1, 11):\n","    kmeans = KMeans(n_clusters=k, random_state=42)\n","    kmeans.fit(X_scaled)\n","    inertias.append(kmeans.inertia_)\n","\n","# エルボーカーブをプロット\n","plt.figure(figsize=(10, 5))\n","plt.plot(range(1, 11), inertias, marker='o')\n","plt.xlabel('Number of clusters (k)')\n","plt.ylabel('Inertia')\n","plt.title('Elbow Method for Optimal k')\n","plt.show()\n","\n","# K-meansクラスタリングを実行（クラスタ数は3に設定）\n","kmeans = KMeans(n_clusters=3, random_state=42)\n","cluster_labels = kmeans.fit_predict(X_scaled)\n","\n","# クラスタリング結果と実際のラベルを比較し、正解率を計算する\n","label_mapping = {}\n","for i in range(3):\n","    mask = (cluster_labels == i)\n","    label_mapping[i] = np.argmax(np.bincount(y[mask]))\n","\n","predicted_labels = np.array([label_mapping[label] for label in cluster_labels])\n","\n","# 正解率を計算\n","accuracy = accuracy_score(y, predicted_labels)\n","\n","# Adjusted Rand Indexを計算\n","# クラスタリングの結果と正解のラベル付けがどれくらい一致しているかを測定\n","ari = adjusted_rand_score(y, cluster_labels)\n","\n","print(f\"K-means Clustering Results:\")\n","print(f\"Number of clusters: 3\")\n","print(f\"Accuracy: {accuracy:.2f}\")\n","print(f\"Adjusted Rand Index: {ari:.2f}\")\n","\n","# PCAを使用して2次元に削減\n","pca = PCA(n_components=2)\n","X_pca = pca.fit_transform(X_scaled)\n","\n","# 結果の可視化\n","plt.figure(figsize=(12, 5))\n","\n","# クラスタリング結果\n","plt.subplot(1, 2, 1)\n","scatter = plt.scatter(X_pca[:, 0], X_pca[:, 1], c=cluster_labels, cmap='viridis')\n","plt.xlabel('First Principal Component')\n","plt.ylabel('Second Principal Component')\n","plt.title(f'K-means Clustering Result\\nAccuracy: {accuracy:.2f}, ARI: {ari:.2f}')\n","plt.colorbar(scatter)\n","\n","# 実際のラベル\n","plt.subplot(1, 2, 2)\n","scatter = plt.scatter(X_pca[:, 0], X_pca[:, 1], c=y, cmap='viridis')\n","plt.xlabel('First Principal Component')\n","plt.ylabel('Second Principal Component')\n","plt.title('Actual Labels')\n","plt.colorbar(scatter)\n","\n","plt.tight_layout()\n","plt.show()"],"metadata":{"id":"GNYliVt-e-Qk"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# 実践15"],"metadata":{"id":"Aj1vut4e8E43"}},{"cell_type":"markdown","source":["## 実践15-1\n","以下のデータはWikipediaページビューデータです。１年先までの時系列予測と評価をおこなってください。\n","\n","\n","```\n","data = pd.read_csv('https://raw.githubusercontent.com/facebook/prophet/main/examples/example_wp_log_peyton_manning.csv')\n","```"],"metadata":{"id":"mvKYDlbm8HlS"}},{"cell_type":"code","source":["import pandas as pd\n","from prophet import Prophet\n","\n","# データの読み込み\n","data = pd.read_csv('https://raw.githubusercontent.com/facebook/prophet/main/examples/example_wp_log_peyton_manning.csv')\n","display(data.head())\n","\n","# モデルの構築\n","model = Prophet()\n","model.fit(data)\n","\n","# 未来データの生成\n","future = model.make_future_dataframe(periods=365, include_history=True)\n","\n","# 予測\n","forecast = model.predict(future)\n","\n","# 評価\n","from prophet.diagnostics import cross_validation\n","\n","df_cv = cross_validation(model, horizon='365 days')\n","print(df_cv.tail())\n","\n","# 結果の可視化\n","model.plot(forecast)\n","model.plot_components(forecast)\n"],"metadata":{"id":"NwL5nBZYfZ3u"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## 実践15-2\n","* https://terakoya.sejuku.net/programs/110/chapters/1381 ではdsとyだけのデータでした。実務では他にも多数の特徴量が存在しています。\n","* 複数の特徴量がある場合、prophetではどのように読み込めばよいでしょうか？調査して報告してください。"],"metadata":{"id":"OE0ko310P03X"}},{"cell_type":"code","source":["import pandas as pd\n","from prophet import Prophet\n","\n","# データの準備\n","df = pd.DataFrame({\n","    'ds': pd.date_range(start='2023-01-01', periods=365),\n","    'y': [100 + i + np.random.normal(0, 5) for i in range(365)],\n","    'feature1': np.random.randn(365),\n","    'feature2': np.random.randn(365)\n","})\n","\n","# モデルの初期化\n","model = Prophet()\n","\n","# regressorsの追加\n","model.add_regressor('feature1')\n","model.add_regressor('feature2')\n","\n","# モデルのフィッティング\n","model.fit(df)\n","\n","# 将来予測用のデータフレーム作成\n","future = model.make_future_dataframe(periods=30)\n","future['feature1'] = np.random.randn(395)  # 365 + 30\n","future['feature2'] = np.random.randn(395)  # 365 + 30\n","\n","# 予測\n","forecast = model.predict(future)"],"metadata":{"id":"P2RI-CyVP2Tr"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## 実践15-3\n","* prophet以外にも著名なライブラリとしてstatsmodel, sktime, darts等があります。\n","* prophet含めそれぞれの利点・欠点を調査し報告してください。"],"metadata":{"id":"b_HtyOlGQAfq"}},{"cell_type":"markdown","source":["各ライブラリの利点と欠点を簡潔に報告いたします。\n","\n","1. Prophet\n","\n","利点:\n","- 使いやすさ：最小限の設定で高品質な予測が可能\n","- 季節性や休日の効果を自動的に処理\n","- ビジネス向けに設計され、解釈しやすい結果を提供\n","\n","欠点:\n","- カスタマイズ性に制限がある\n","- 複雑なモデルには適さない\n","- 大規模データセットでは処理速度が遅くなる可能性がある\n","\n","\n","2. Statsmodels\n","\n","利点:\n","- 統計モデルの幅広いコレクション（ARIMA, VAR, 回帰分析など）\n","- 詳細な統計結果と診断ツールを提供\n","- 学術研究や高度な統計分析に適している\n","\n","欠点:\n","- 初心者にとっては学習曲線が急\n","- 自動化された予測機能が少ない\n","- データ前処理に多くの手動作業が必要\n","\n","3. Sktime\n","\n","利点:\n","- 機械学習ベースの時系列分析に特化\n","- scikit-learnとの互換性が高い\n","- 複数の時系列タスク（分類、回帰、予測）に対応\n","\n","欠点:\n","- 比較的新しいライブラリで、ドキュメントが限られている\n","- 一部の高度な機能はまだ開発中\n","- 統計的アプローチよりも機械学習に重点を置いている\n","\n","4. Darts\n","\n","利点:\n","- 多様なモデル（統計的、機械学習、ディープラーニング）をサポート\n","- バックテスティングや交差検証の機能が充実\n","- 複数の時系列を同時に扱える\n","\n","欠点:\n","- 学習曲線がやや急\n","- 一部の高度な機能には追加のライブラリのインストールが必要\n","- Prophetほど自動化されていない面がある\n","\n","これらのライブラリは、それぞれ異なる強みと弱みを持っています。選択は具体的なプロジェクトの要件、データの性質、ユーザーのスキルレベルに応じて行いましょう。"],"metadata":{"id":"Px46eJwFQJ3H"}},{"cell_type":"markdown","source":["＃# 実践15-4: パラメータ調査\n","* https://raw.githubusercontent.com/facebook/prophet/main/examples/example_wp_log_peyton_manning.csv の分析ではseasonality_mode='multiplicative'で効果がありました。それ以外に効果がありそうな設定を調査し試してください。\n","* 参考(以下を設定したから精度が上がるとは限りません)\n","\n","1. 休日の効果を考慮:\n","    * add_country_holidays(country_name='US')を使用して、アメリカの休日を考慮に入れます。\n","2. weekly_seasonalityの設定 (現在は月次なのでどうすべきか)\n","3. n_changepointsの設定\n","    * データ内でトレンドが変化するポイントの候補数\n","    * デフォルト設定:\n","\n","    デフォルトでは、Prophetはデータの最初の80%の期間内に、25個の変化点を均等に配置します。\n","    例えば、データが1000日分ある場合、最初の800日間に25個の変化点候補が配置されます。\n","    * 使用例:\n","\n","    値を増やすことで、モデルはより多くの変化点を検討するため、トレンドの急激な変動を捉えやすくなります。\n","    値を減らすと、モデルはトレンドをよりスムーズにし、大きな変動を無視する傾向になります。\n","4. 外部リグレッサの追加: 外部の経済指標等を別カラムに入れる\n","5. その他あれば\n"],"metadata":{"id":"ZyWkZzxeQSnQ"}}]}