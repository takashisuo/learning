{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyPCzdMxt1eBndGbjmX5qI/Q"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["前回と同じリポジトリ(nlp)にjupyterを格納してください。"],"metadata":{"id":"-PJiP3Jqv3_7"}},{"cell_type":"code","source":["!pip install gensim mecab-python3 unidic-lite pandas nltk scikit-learn"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"btqu4CuM4MwY","executionInfo":{"status":"ok","timestamp":1719026443368,"user_tz":-540,"elapsed":6750,"user":{"displayName":"Takashi Suou","userId":"04024765498720654612"}},"outputId":"8d0ec7eb-a115-4de5-ada2-5370faca77ce"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: gensim in /usr/local/lib/python3.10/dist-packages (4.3.2)\n","Requirement already satisfied: mecab-python3 in /usr/local/lib/python3.10/dist-packages (1.0.9)\n","Requirement already satisfied: unidic-lite in /usr/local/lib/python3.10/dist-packages (1.0.8)\n","Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (2.0.3)\n","Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.8.1)\n","Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (1.2.2)\n","Requirement already satisfied: numpy>=1.18.5 in /usr/local/lib/python3.10/dist-packages (from gensim) (1.25.2)\n","Requirement already satisfied: scipy>=1.7.0 in /usr/local/lib/python3.10/dist-packages (from gensim) (1.11.4)\n","Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.10/dist-packages (from gensim) (7.0.4)\n","Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas) (2.8.2)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2023.4)\n","Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.1)\n","Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.7)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.4.2)\n","Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2024.5.15)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.66.4)\n","Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (3.5.0)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n","Requirement already satisfied: wrapt in /usr/local/lib/python3.10/dist-packages (from smart-open>=1.8.1->gensim) (1.14.1)\n"]}]},{"cell_type":"markdown","source":["## 問題4-1. Bag Of Words (テキスト外), mecabによる品詞分解\n","* Bag Of Wordsについて調べて理解したことを記載してください。\n","* 以下のテキストを利用してください。\n","```\n","text = \"\"\"\n","私は昨日、新しい本を買いました。\n","その本はとても面白くて、一気に読み終えました。\n","読書は私の大好きな趣味の一つです。\n","新しい知識を得られるので、毎日少しずつ本を読むようにしています。\n","\"\"\"\n","```\n","* Bag Of Wordsを算出してください。そのためにmecabを用いて'名詞', '動詞', '形容詞'のみを抽出し計測してください。\n"],"metadata":{"id":"2f2r7Sl22_1J"}},{"cell_type":"markdown","source":["## 問題4-2. N-Gram (テキスト外)\n","1. N-Gramについて調査してください。\n","2. 以下の文書でBigramを算出してください。\n","\n","```\n","text = \"自然言語処理はとても面白い分野です。多くの応用があり、可能性は無限です。\"\n","```\n","3. ヒント: ngram計算には形態素解析で分解する必要があります。\n","4. ヒント: Bigramはfrom nltk.util import ngramsを利用すると簡単にできます。\n"],"metadata":{"id":"qeZkofZa35iV"}},{"cell_type":"markdown","source":["* Unigram (1-gram): 単一の単語（または文字）。\n","    例: \"I love NLP\" では、Unigram は [\"I\", \"love\", \"NLP\"] です。\n","\n","* Bigram (2-gram): 連続する2つの単語（または文字）の組み合わせ。\n","    例: \"I love NLP\" では、Bigram は [\"I love\", \"love NLP\"] です。\n","\n","* Trigram (3-gram): 連続する3つの単語（または文字）の組み合わせ。\n","    例: \"I love NLP\" では、Trigram は [\"I love NLP\"] です。"],"metadata":{"id":"h324FG5K6KdM"}},{"cell_type":"markdown","source":["## 問題4-3. 前処理・形態素解析・Word2Vec・cos類似度 (応用)\n","1. 以下のサンプルデータを用いたdataframeを作成してください。\n","```\n","# サンプルの日本語製品レビューのデータ\n","data = {\n","    'review_id': [1, 2, 3, 4, 5],\n","    'review_text': [\n","        \"この商品が大好きです。とても使いやすいです。\",\n","        \"最悪の商品です。絶対にお勧めしません。\",\n","        \"価格の割に良い品質です。満足しています。\",\n","        \"もう一度購入したいと思います。とても良いです。\",\n","        \"耐久性がありません。すぐに壊れてしまいました。\"\n","    ]\n","df = pd.DataFrame(data)\n","}\n","```\n","1. レビュー箇所の前処理としてdef preprocess_text(text):を定義し実行してください。基本的に形態素解析した結果を別カラムに入れる想定です。\n","1. Word2Vecモデルの学習を実施してください。\n","1. 各レビューの文書ベクトルを計算し、dataframeの\"vector\"カラムに入れてください。\n","1. (応用) 文書ベクトルのリストを取り出してコサイン類似度を計算してください。その結果をindex=df['review_id'], columns=df['review_id']のdataframeに格納してください。\n"],"metadata":{"id":"gQ9QWrkbwBH7"}},{"cell_type":"markdown","source":["#問題4-4. TF-IDF\n","1. 以下のデータを利用してdataframeを作成してください。\n","\n","\n","```\n","data = {\n","    'review_id': [1, 2, 3, 4, 5],\n","    'review_text': [\n","        \"この商品が大好きです。とても使いやすいです。\",\n","        \"最悪の商品です。絶対にお勧めしません。\",\n","        \"価格の割に良い品質です。満足しています。\",\n","        \"もう一度購入したいと思います。とても良いです。\",\n","        \"耐久性がありません。すぐに壊れてしまいました。\"\n","    ]\n","}\n","\n","# データフレームの作成と保存\n","df = pd.DataFrame(data)\n","```\n","1. このdataframeを用いて各レビューでのtf-idfを算出してdataframeの別カラムに格納してください。\n","1. tf-idfのdataframeを作成してください。index=review_id, カラム: 分割した単語, 値: tf-idf値\n"],"metadata":{"id":"X_x2G4amDd8h"}},{"cell_type":"code","source":[],"metadata":{"id":"NGds_N1VD9ZJ"},"execution_count":null,"outputs":[]}]}